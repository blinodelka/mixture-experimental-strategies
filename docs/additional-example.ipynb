{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autora\n",
    "import numpy as np\n",
    "from typing import Iterable, Literal, Optional\n",
    "\n",
    "from sklearn.metrics import DistanceMetric\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch \n",
    "from torch.autograd import Variable\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autora.experimentalist.pooler.poppernet import (\n",
    "    PopperNet,\n",
    "    class_to_onehot,\n",
    "    plot_popper_diagnostics,\n",
    ")\n",
    "from autora.variable import ValueType, VariableCollection\n",
    "\n",
    "\n",
    "def falsification_sampler(\n",
    "    X,\n",
    "    model,\n",
    "    X_train: np.ndarray,\n",
    "    Y_train: np.ndarray,\n",
    "    metadata: VariableCollection,\n",
    "    n: Optional[int] = None,\n",
    "    training_epochs: int = 1000,\n",
    "    training_lr: float = 1e-3,\n",
    "    plot: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    A Sampler that generates samples for independent variables with the objective of maximizing the\n",
    "    (approximated) loss of the model. The samples are generated by first training a neural network\n",
    "    to approximate the loss of a model for all patterns in the training data.\n",
    "    Once trained, the network is then provided with the candidate samples and the samples with\n",
    "    the highest loss are selected.\n",
    "\n",
    "    Args:\n",
    "        X: The candidate samples of conditions to be evaluated.\n",
    "        model: Scikit-learn model, could be either a classification or regression model\n",
    "        X_train: Conditions that the model was trained on\n",
    "        Y_train: Observations that the model was trained to predict\n",
    "        metadata: Meta-data about the dependent and independent variables\n",
    "        n: Number of samples to return\n",
    "        training_epochs: Number of epochs to train the popper network for approximating the\n",
    "        error of the model\n",
    "        training_lr: Learning rate for training the popper network\n",
    "        plot: Print out the prediction of the popper network as well as its training loss\n",
    "\n",
    "    Returns: Samples with the highest loss\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # format input\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    if len(X_train.shape) == 1:\n",
    "        X_train = X_train.reshape(-1, 1)\n",
    "\n",
    "    # get target pattern for popper net\n",
    "    model_predict = getattr(model, \"predict_proba\", None)\n",
    "    if callable(model_predict) is False:\n",
    "        model_predict = getattr(model, \"predict\", None)\n",
    "\n",
    "    if callable(model_predict) is False or model_predict is None:\n",
    "        raise Exception(\"Model must have `predict` or `predict_proba` method.\")\n",
    "\n",
    "    model_prediction = model_predict(X_train)\n",
    "    if isinstance(model_prediction, np.ndarray) is False:\n",
    "        try:\n",
    "            model_prediction = np.array(model_prediction)\n",
    "        except Exception:\n",
    "            raise Exception(\"Model prediction must be convertable to numpy array.\")\n",
    "    if model_prediction.ndim == 1:\n",
    "        model_prediction = model_prediction.reshape(-1, 1)\n",
    "\n",
    "    return get_samples_from_model_prediction(\n",
    "        X,\n",
    "        model_prediction,\n",
    "        X_train,\n",
    "        Y_train,\n",
    "        metadata,\n",
    "        n,\n",
    "        training_epochs,\n",
    "        training_lr,\n",
    "        plot,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_samples_from_model_prediction(\n",
    "    X,\n",
    "    Y_predicted,\n",
    "    X_train: np.ndarray,\n",
    "    Y_train: np.ndarray,\n",
    "    metadata: Optional[VariableCollection] = None,\n",
    "    n: Optional[int] = None,\n",
    "    training_epochs: int = 1000,\n",
    "    training_lr: float = 1e-3,\n",
    "    plot: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    A Sampler that generates samples for independent variables with the objective of maximizing the\n",
    "    (approximated) loss of the model. The samples are generated by first training a neural network\n",
    "    to approximate the loss of a model for all patterns in the training data. The model\n",
    "    loss is computed based on the predictions of the model for its training data. Once trained,\n",
    "    the network is then provided with the candidate samples and the samples with the highest loss\n",
    "    are selected.\n",
    "\n",
    "    Args:\n",
    "        X: The candidate samples to be evaluated.\n",
    "        Y_predicted: Prediction obtained from the model for the set of conditions X_train\n",
    "        X_train: Conditions that the model was trained on\n",
    "        Y_train: Observations that the model was trained to predict\n",
    "        metadata: Meta-data about the dependent and independent variables\n",
    "        n: Number of samples to return\n",
    "        training_epochs: Number of epochs to train the popper network for approximating the\n",
    "        error of the model\n",
    "        training_lr: Learning rate for training the popper network\n",
    "        plot: Print out the prediction of the popper network as well as its training loss\n",
    "\n",
    "    Returns: Samples with the highest loss\n",
    "\n",
    "    \"\"\"\n",
    "    X, scores = get_scored_samples_from_model_prediction(\n",
    "        X,\n",
    "        Y_predicted,\n",
    "        X_train,\n",
    "        Y_train,\n",
    "        metadata,\n",
    "        n,\n",
    "        training_epochs,\n",
    "        training_lr,\n",
    "        plot,\n",
    "    )\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def get_scored_samples_from_model_prediction(\n",
    "    X,\n",
    "    Y_predicted,\n",
    "    X_train: np.ndarray,\n",
    "    Y_train: np.ndarray,\n",
    "    metadata: Optional[VariableCollection] = None,\n",
    "    n: Optional[int] = None,\n",
    "    training_epochs: int = 1000,\n",
    "    training_lr: float = 1e-3,\n",
    "    plot: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    A Sampler that generates samples for independent variables with the objective of maximizing the\n",
    "    (approximated) loss of the model. The samples are generated by first training a neural network\n",
    "    to approximate the loss of a model for all patterns in the training data. The model\n",
    "    loss is computed based on the predictions of the model for its training data. Once trained,\n",
    "    the network is then provided with the candidate samples and the samples with the highest loss\n",
    "    are selected.\n",
    "\n",
    "    Args:\n",
    "        X: The candidate samples to be evaluated.\n",
    "        Y_predicted: Prediction obtained from the model for the set of conditions X_train\n",
    "        X_train: Conditions that the model was trained on\n",
    "        Y_train: Observations that the model was trained to predict\n",
    "        metadata: Meta-data about the dependent and independent variables\n",
    "        n: Number of samples to return\n",
    "        training_epochs: Number of epochs to train the popper network for approximating the\n",
    "        error of the model\n",
    "        training_lr: Learning rate for training the popper network\n",
    "        plot: Print out the prediction of the popper network as well as its training loss\n",
    "\n",
    "    Returns:\n",
    "        X: Samples with the highest loss\n",
    "        scores: Normalized falsification scores for the samples\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    X = np.array(X)\n",
    "    if len(X.shape) == 1:\n",
    "        X = X.reshape(-1, 1)\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    if len(X_train.shape) == 1:\n",
    "        X_train = X_train.reshape(-1, 1)\n",
    "\n",
    "    Y_train = np.array(Y_train)\n",
    "    if len(Y_train.shape) == 1:\n",
    "        Y_train = Y_train.reshape(-1, 1)\n",
    "\n",
    "    if n is None:\n",
    "        n = X.shape[0]\n",
    "\n",
    "    if metadata is not None:\n",
    "        if metadata.dependent_variables[0].type == ValueType.CLASS:\n",
    "            # find all unique values in Y_train\n",
    "            num_classes = len(np.unique(Y_train))\n",
    "            Y_train = class_to_onehot(Y_train, n_classes=num_classes)\n",
    "\n",
    "    # create list of IV limits\n",
    "    iv_limit_list = list()\n",
    "    if metadata is not None:\n",
    "        ivs = metadata.independent_variables\n",
    "        for iv in ivs:\n",
    "            if hasattr(iv, \"value_range\"):\n",
    "                value_range = cast(Tuple, iv.value_range)\n",
    "                lower_bound = value_range[0]\n",
    "                upper_bound = value_range[1]\n",
    "                iv_limit_list.append(([lower_bound, upper_bound]))\n",
    "    else:\n",
    "        for col in range(X.shape[1]):\n",
    "            min = np.min(X[:, col])\n",
    "            max = np.max(X[:, col])\n",
    "            iv_limit_list.append(([min, max]))\n",
    "\n",
    "    # get input pattern for popper net\n",
    "    popper_input = Variable(torch.from_numpy(X_train), requires_grad=False).float()\n",
    "\n",
    "    # get dimensions of input and output\n",
    "    n_input = X_train.shape[1]\n",
    "    n_output = 1  # only predicting one MSE\n",
    "\n",
    "    if isinstance(Y_predicted, np.ndarray) is False:\n",
    "        try:\n",
    "            Y_predicted = np.array(Y_predicted)\n",
    "        except Exception:\n",
    "            raise Exception(\"Model prediction must be convertable to numpy array.\")\n",
    "    if Y_predicted.ndim == 1:\n",
    "        Y_predicted = Y_predicted.reshape(-1, 1)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    model_loss = (Y_predicted - Y_train) ** 2\n",
    "    model_loss = np.mean(model_loss, axis=1)\n",
    "\n",
    "    # standardize the loss\n",
    "    scaler = StandardScaler()\n",
    "    model_loss = scaler.fit_transform(model_loss.reshape(-1, 1)).flatten()\n",
    "\n",
    "    model_loss = torch.from_numpy(model_loss).float()\n",
    "    popper_target = Variable(model_loss, requires_grad=False)\n",
    "\n",
    "    # create the network\n",
    "    popper_net = PopperNet(n_input, n_output)\n",
    "\n",
    "    # reformat input in case it is 1D\n",
    "    if len(popper_input.shape) == 1:\n",
    "        popper_input = popper_input.flatten()\n",
    "        popper_input = popper_input.reshape(-1, 1)\n",
    "\n",
    "    # define the optimizer\n",
    "    popper_optimizer = torch.optim.Adam(popper_net.parameters(), lr=training_lr)\n",
    "\n",
    "    # train the network\n",
    "    losses = []\n",
    "    for epoch in range(training_epochs):\n",
    "        popper_prediction = popper_net(popper_input)\n",
    "        loss = criterion(popper_prediction, popper_target.reshape(-1, 1))\n",
    "        popper_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        popper_optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    if plot:\n",
    "        popper_input_full = np.linspace(\n",
    "            iv_limit_list[0][0], iv_limit_list[0][1], 1000\n",
    "        ).reshape(-1, 1)\n",
    "        popper_input_full = Variable(\n",
    "            torch.from_numpy(popper_input_full), requires_grad=False\n",
    "        ).float()\n",
    "        popper_prediction = popper_net(popper_input_full)\n",
    "        plot_popper_diagnostics(\n",
    "            losses,\n",
    "            popper_input,\n",
    "            popper_input_full,\n",
    "            popper_prediction,\n",
    "            popper_target,\n",
    "            Y_predicted,\n",
    "            Y_train,\n",
    "        )\n",
    "\n",
    "    # now that the popper network is trained we can assign losses to all data points to be evaluated\n",
    "    popper_input = Variable(torch.from_numpy(X)).float()\n",
    "    Y = popper_net(popper_input).detach().numpy().flatten()\n",
    "    scaler = StandardScaler()\n",
    "    score = scaler.fit_transform(Y.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # order rows in Y from highest to lowest\n",
    "    sorted_X = X[np.argsort(score)[::-1]]\n",
    "    sorted_score = score[np.argsort(score)[::-1]]\n",
    "\n",
    "    return sorted_X[:n], sorted_score[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllowedMetrics = Literal[\n",
    "    \"euclidean\",\n",
    "    \"manhattan\",\n",
    "    \"chebyshev\",\n",
    "    \"minkowski\",\n",
    "    \"wminkowski\",\n",
    "    \"seuclidean\",\n",
    "    \"mahalanobis\",\n",
    "    \"haversine\",\n",
    "    \"hamming\",\n",
    "    \"canberra\",\n",
    "    \"braycurtis\",\n",
    "    \"matching\",\n",
    "    \"jaccard\",\n",
    "    \"dice\",\n",
    "    \"kulsinski\",\n",
    "    \"rogerstanimoto\",\n",
    "    \"russellrao\",\n",
    "    \"sokalmichener\",\n",
    "    \"sokalsneath\",\n",
    "    \"yule\",\n",
    "]\n",
    "\n",
    "\n",
    "def summed_dissimilarity_sampler(\n",
    "    X: np.ndarray,\n",
    "    X_ref: np.ndarray,\n",
    "    n: int = 1,\n",
    "    metric: AllowedMetrics = \"euclidean\",\n",
    "    integration: str = \"min\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    This dissimilarity samples re-arranges the pool of IV conditions according to their\n",
    "    dissimilarity with respect to a reference pool X_ref. The default dissimilarity is calculated\n",
    "    as the average of the pairwise distances between the conditions in X and X_ref.\n",
    "\n",
    "    Args:\n",
    "        X: pool of IV conditions to evaluate dissimilarity\n",
    "        X_ref: reference pool of IV conditions\n",
    "        n: number of samples to select\n",
    "        metric (str): dissimilarity measure. Options: 'euclidean', 'manhattan', 'chebyshev',\n",
    "            'minkowski', 'wminkowski', 'seuclidean', 'mahalanobis', 'haversine',\n",
    "            'hamming', 'canberra', 'braycurtis', 'matching', 'jaccard', 'dice',\n",
    "            'kulsinski', 'rogerstanimoto', 'russellrao', 'sokalmichener',\n",
    "            'sokalsneath', 'yule'. See [sklearn.metrics.DistanceMetric][] for more details.\n",
    "\n",
    "    Returns:\n",
    "        Sampled pool\n",
    "    \"\"\"\n",
    "\n",
    "    new_X, distance_scores = compute_dissimilarity(X, X_ref, n, metric, integration)\n",
    "\n",
    "    return new_X\n",
    "\n",
    "\n",
    "def compute_dissimilarity(\n",
    "    X: np.ndarray,\n",
    "    X_ref: np.ndarray,\n",
    "    n: int = 1,\n",
    "    metric: AllowedMetrics = \"euclidean\",\n",
    "    integration: str = \"min\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    This dissimilarity samples re-arranges the pool of IV conditions according to their\n",
    "    dissimilarity with respect to a reference pool X_ref. The default dissimilarity is calculated\n",
    "    as the average of the pairwise distances between the conditions in X and X_ref.\n",
    "\n",
    "    Args:\n",
    "        X: pool of IV conditions to evaluate dissimilarity\n",
    "        X_ref: reference pool of IV conditions\n",
    "        n: number of samples to select\n",
    "        metric (str): dissimilarity measure. Options: 'euclidean', 'manhattan', 'chebyshev',\n",
    "            'minkowski', 'wminkowski', 'seuclidean', 'mahalanobis', 'haversine',\n",
    "            'hamming', 'canberra', 'braycurtis', 'matching', 'jaccard', 'dice',\n",
    "            'kulsinski', 'rogerstanimoto', 'russellrao', 'sokalmichener',\n",
    "            'sokalsneath', 'yule'. See [sklearn.metrics.DistanceMetric][] for more details.\n",
    "        integration: Distance integration method used to compute the overall dissimilarity score\n",
    "        for a given data point. Options: 'sum', 'prod', 'mean', 'min', 'max'.\n",
    "\n",
    "    Returns:\n",
    "        Sampled pool\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(X, Iterable):\n",
    "        X = np.array(list(X))\n",
    "\n",
    "    if isinstance(X_ref, Iterable):\n",
    "        X_ref = np.array(list(X_ref))\n",
    "\n",
    "    if X.ndim == 1:\n",
    "        X = X.reshape(-1, 1)\n",
    "\n",
    "    if X_ref.ndim == 1:\n",
    "        X_ref = X_ref.reshape(-1, 1)\n",
    "\n",
    "    if X.shape[1] != X_ref.shape[1]:\n",
    "        raise ValueError(\n",
    "            f\"X and X_ref must have the same number of columns.\\n\"\n",
    "            f\"X has {X.shape[1]} columns, while X_ref has {X_ref.shape[1]} columns.\"\n",
    "        )\n",
    "\n",
    "    if X.shape[0] < n:\n",
    "        raise ValueError(\n",
    "            f\"X must have at least {n} rows matching the number of requested samples.\"\n",
    "        )\n",
    "\n",
    "    dist = DistanceMetric.get_metric(metric)\n",
    "\n",
    "    distances = dist.pairwise(X_ref, X)\n",
    "\n",
    "    if integration == \"sum\":\n",
    "        integrated_distance = np.sum(distances, axis=0)\n",
    "    elif integration == \"mean\":\n",
    "        integrated_distance = np.mean(distances, axis=0)\n",
    "    elif integration == \"max\":\n",
    "        integrated_distance = np.max(distances, axis=0)\n",
    "    elif integration == \"min\":\n",
    "        integrated_distance = np.min(distances, axis=0)\n",
    "    elif integration == \"prod\":\n",
    "        integrated_distance = np.prod(distances, axis=0)\n",
    "    else:\n",
    "        raise ValueError(f\"Integration method {integration} not supported.\")\n",
    "\n",
    "    # normalize the distances\n",
    "    scaler = StandardScaler()\n",
    "    score = scaler.fit_transform(integrated_distance.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # order rows in Y from highest to lowest\n",
    "    sorted_X = X[np.argsort(integrated_distance)[::-1]]\n",
    "    sorted_score = score[np.argsort(score)[::-1]]\n",
    "\n",
    "    return sorted_X[:n], sorted_score[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_distribution(p, temperature):\n",
    "        # temperature cannot be 0\n",
    "        #If the temperature is very low (close to 0), then the sampling will become almost deterministic, picking the event with the highest probability.\n",
    "        #If the temperature is very high, then the sampling will be closer to uniform, with all events having roughly equal probability.\n",
    "        \n",
    "        p = p / np.sum(p)  # Normalizing the initial distribution\n",
    "        p = np.log(p) / temperature  \n",
    "        p = np.exp(p)  \n",
    "        p = p / np.sum(p) # Normalizing the final distribution\n",
    "        return p\n",
    "\n",
    "\n",
    "    \n",
    "def mixture_sampler(\n",
    "    condition_pool: np.ndarray, weights: np.ndarray, temperature: int, \n",
    "    X_ref: np.ndarray, \n",
    "    X_train: np.ndarray,\n",
    "    Y_train: np.ndarray, Y_predicted, num_samples: Optional[int] = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Add a description of the sampler here.\n",
    "\n",
    "    Args:\n",
    "        condition_pool: pool of experimental conditions to evaluate\n",
    "        num_samples: number of experimental conditions to select\n",
    "        weights: array containing 4 weights -- importance of the falsification, confirmation, novelty, and familiarity (ideally, each pair of opposites? sums up to 1 or all? sum up to 1)\n",
    "        temperature: how random is selection of conditions (cannot be 0; (0:1) - the choices are more deterministic than the choices made wrt\n",
    "        the mixture scores; 1 - choices are made wrt to the mixture scores; (1, inf) - the choices are more random)\n",
    "        X_ref, X_train, Y_train, Y_predicted: parameters required for falsification and novelty samplers\n",
    "    \n",
    "    Returns:\n",
    "        Sampled pool of experimental conditions\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    falsification_ranking, falsification_scores = get_scored_samples_from_model_prediction(condition_pool, \n",
    "                                                                                           Y_predicted, X_train,\n",
    "                                                                                           Y_train, n=condition_pool.shape[0])\n",
    "    \n",
    "    # getting rid of negative scores by introducing confirmation scores \n",
    "    confirmation_scores = -falsification_scores\n",
    "    confirmation_scores[falsification_scores>0]=0\n",
    "    falsification_scores[falsification_scores<0]=0\n",
    "    \n",
    "    # getting rid of negative scores by introducing familiarity scores \n",
    "    novelty_ranking, novelty_scores = compute_dissimilarity(condition_pool, X_ref, n=condition_pool.shape[0])\n",
    "    \n",
    "    familiarity_scores = -novelty_scores\n",
    "    familiarity_scores[novelty_scores>0]=0\n",
    "    novelty_scores[novelty_scores<0]=0\n",
    "    \n",
    "    # aligning the arrays based on the observations (condition pools)\n",
    "    novelty_indices = np.argsort(novelty_ranking, axis=None)\n",
    "    ranking_sorted = novelty_ranking[novelty_indices]\n",
    "    novelty_scores_sorted = novelty_scores[novelty_indices]\n",
    "    familiarity_scores_sorted = familiarity_scores[novelty_indices]\n",
    "\n",
    "    falsification_indices = np.argsort(falsification_ranking, axis=None)\n",
    "    falsification_scores_sorted = falsification_scores[falsification_indices]    \n",
    "    confirmation_scores_sorted = confirmation_scores[falsification_indices] \n",
    "    \n",
    "    weighted_mixture_scores = falsification_scores_sorted * weights[0] + confirmation_scores_sorted * weights[1] + novelty_scores_sorted * weights[2] + familiarity_scores_sorted * weights[3] \n",
    "    # each score is weighted by the relative importance of these different axes\n",
    "    \n",
    "    \n",
    "    # adjust mixture scores wrt temperature\n",
    "    weighted_mixture_scores_adjusted = adjust_distribution(weighted_mixture_scores, temperature)\n",
    "    \n",
    "    if num_samples is None:\n",
    "        num_samples = condition_pool.shape[0]\n",
    "    \n",
    "    conditions = np.random.choice(ranking_sorted.T.squeeze(), num_samples,\n",
    "              p=weighted_mixture_scores_adjusted, replace = False)\n",
    "    \n",
    "    return conditions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtureExperimentalist:\n",
    "    def __init__(self, weights: np.ndarray, temperature: int, \n",
    "    X_ref: np.ndarray, \n",
    "    X_train: np.ndarray,\n",
    "    Y_train: np.ndarray, Y_predicted, num_samples: Optional[int] = None):\n",
    "        self.weights = weights\n",
    "        self.temperature = temperature\n",
    "        self.X_ref = X_ref\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "        self.Y_predicted = Y_predicted\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __call__(self, condition_pool: np.ndarray, **kwargs):\n",
    "        params = dict(weights = self.weights, temperature = self.temperature, X_ref = self.X_ref,\n",
    "                     X_train = self.X_train, Y_train = self.Y_train, Y_predicted = self.Y_predicted,\n",
    "                     num_samples = self.num_samples)\n",
    "        params.update(kwargs)\n",
    "        \n",
    "        samples = mixture_sampler(condition_pool, **params)\n",
    "        return samples\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentalist = MixtureExperimentalist(weights = np.array([0.2,0.3,0.3,0.7]), temperature = 20, \n",
    "    X_ref = np.array([10,25,30]), \n",
    "    X_train = np.array([2,5,6]),\n",
    "    Y_train = np.array([20,25,26]), Y_predicted = np.array([21,22,23]), num_samples = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[[1]\n",
      " [2]\n",
      " [3]]\n",
      "(3, 1)\n",
      "[0.35493798 0.28306655 0.36199547]\n",
      "(3,)\n",
      "[1 2 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 3, 2])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experimentalist(condition_pool = np.array([1,2,3]), num_samples = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[[1]\n",
      " [2]\n",
      " [3]]\n",
      "(3, 1)\n",
      "[0.3509837  0.29130689 0.3577094 ]\n",
      "(3,)\n",
      "[1 2 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 3])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixture_sampler(\n",
    "    np.array([1,2,3]), np.array([0.2,0.3,0.3,0.7]), 20, \n",
    "    np.array([10,25,30]), \n",
    "    np.array([2,5,6]),\n",
    "    np.array([20,25,26]), np.array([21,22,23]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
